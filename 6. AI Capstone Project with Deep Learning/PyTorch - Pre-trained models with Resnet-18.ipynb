{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><h1>Pre-trained-Models with PyTorch </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
    "<ul>\n",
    "<li>change the output layer</li>\n",
    "<li> train the model</li> \n",
    "<li>  identify  several  misclassified samples</li> \n",
    " </ul>\n",
    "You will take several screenshots of your work and share your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-20 10:30:56--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2598656062 (2.4G) [application/zip]\n",
      "Saving to: ‘Positive_tensors.zip’\n",
      "\n",
      "100%[====================================>] 2,598,656,062 51.1MB/s   in 54s    \n",
      "\n",
      "2020-02-20 10:31:50 (46.1 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-20 10:33:49--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2111408108 (2.0G) [application/zip]\n",
      "Saving to: ‘Negative_tensors.zip’\n",
      "\n",
      "100%[====================================>] 2,111,408,108 46.5MB/s   in 45s    \n",
      "\n",
      "2020-02-20 10:34:34 (44.5 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
    "!unzip -q Negative_tensors.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install torchvision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 8.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\n",
      "Collecting torch==1.4.0 (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 23kB/s s eta 0:00:01MB/s eta 0:00:11        | 288.7MB 558kB/s eta 0:13:52          | 339.9MB 54.0MB/s eta 0:00:08:00:06\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1fea403f50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import pandas\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create your own dataset object\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        directory=\"\"\n",
    "        positive=\"Positive_tensors\"\n",
    "        negative='Negative_tensors'\n",
    "\n",
    "        positive_file_path=os.path.join(directory,positive)\n",
    "        negative_file_path=os.path.join(directory,negative)\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\n",
    "        self.all_files=[None]*number_of_samples\n",
    "        self.all_files[::2]=positive_files\n",
    "        self.all_files[1::2]=negative_files \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        #torch.LongTensor\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2]=1\n",
    "        self.Y[1::2]=0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files=self.all_files[0:30000]\n",
    "            self.Y=self.Y[0:30000]\n",
    "            self.len=len(self.all_files)\n",
    "        else:\n",
    "            self.all_files=self.all_files[30000:]\n",
    "            self.Y=self.Y[30000:]\n",
    "            self.len=len(self.all_files)     \n",
    "       \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "               \n",
    "        image=torch.load(self.all_files[idx])\n",
    "        y=self.Y[idx]\n",
    "                  \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dataset objects, one for the training data and one for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train=True)\n",
    "validation_dataset = Dataset(train=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)\n",
    "print(validation_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_1\">Question 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a pre-trained resnet18 model :</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb6b172295c4c2d90b44f639730c381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the pre-trained model resnet18\n",
    "\n",
    "model_resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
    "for param in model_resnet18.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_resnet18)  ##Take Screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will train your, model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Create a cross entropy criterion function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 100)\n",
    "validation_loader = DataLoader(dataset = validation_dataset, batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Use the following optimizer to minimize the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([parameters  for parameters in model_resnet18.parameters() if parameters.requires_grad],lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "correct=0\n",
    "N_test=len(validation_dataset)\n",
    "N_train=len(train_dataset)\n",
    "start_time = time.time()\n",
    "#n_epochs\n",
    "\n",
    "Loss=0\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        model_resnet18.train() \n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z = model_resnet18(x)\n",
    "        # calculate loss \n",
    "        loss = criterion(z, y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.data)\n",
    "        \n",
    "    correct=0\n",
    "    for x_test, y_test in validation_loader:\n",
    "        # set model to eval \n",
    "        model_resnet18.eval()\n",
    "        #make a prediction \n",
    "        z = model_resnet18(x_test)\n",
    "        #find max \n",
    "        _, label=torch.max(z, 1)\n",
    "       \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint +=(yhat==y_test).sum().item()\n",
    "        correct += (label == y_test).sum().item()\n",
    "        #accuracytemp = 100 * (correct / len(validation_dataset))\n",
    "        #accuracytemp=correct/N_test\n",
    "        #print(str(count)+') accuracy: '+str(accuracytemp))\n",
    "   \n",
    "    accuracy=correct/N_test\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9942\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title(\"The Loss Function\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Identify the first four misclassified samples using the validation data:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) indx: 86 predicted value: tensor(0) actual value: tensor(1)\n",
      "2) indx: 430 predicted value: tensor(0) actual value: tensor(1)\n",
      "3) indx: 433 predicted value: tensor(1) actual value: tensor(0)\n",
      "4) indx: 492 predicted value: tensor(0) actual value: tensor(1)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "batchcount=0\n",
    "incorrect=0\n",
    "for x, y in validation_loader:\n",
    "    if (count<4):\n",
    "        # set model to eval \n",
    "        model_resnet18.eval() \n",
    "       \n",
    "        #make a prediction \n",
    "        z = model_resnet18(x.view(-1, 3, 224, 224))\n",
    "        \n",
    "        #find max \n",
    "        _, label=torch.max(z, 1)\n",
    "       \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint +=(yhat==y_test).sum().item()\n",
    "        correct += (label == y).sum().item()\n",
    "        incorrect += (label != y).sum().item()\n",
    "        #print(label)\n",
    "        #print(y)\n",
    "        #print(correct)\n",
    "        #print('total # of missclassified samples :'+str(incorrect))\n",
    "        for i in range(0,100):\n",
    "            if (label[i]!=y[i]):\n",
    "                count += 1\n",
    "                print(str(count)+') indx: '+str(batchcount+i),'predicted value: '+str(label[i])+' actual value: '+str(y[i]))\n",
    "        batchcount+=100\n",
    "    else:\n",
    "        print(\"done\")\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#: 87 - predicted value: 0 - actual value: 1\n",
      "sample#: 431 - predicted value: 0 - actual value: 1\n",
      "sample#: 434 - predicted value: 1 - actual value: 0\n",
      "sample#: 493 - predicted value: 0 - actual value: 1\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "sampleSeq=0\n",
    "N_samples=0\n",
    "\n",
    "for x_test, y_test in validation_loader:\n",
    "    model_resnet18.eval()\n",
    "    z=model_resnet18(x_test)\n",
    "    _, yhat=torch.max(z.data,1)\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        sampleSeq += 1\n",
    "        if yhat[i] != y_test[i]:\n",
    "            print(\"sample#: %d - predicted value: %d - actual value: %d\" % (sampleSeq, yhat[i], y_test[i]))\n",
    "            N_samples += 1\n",
    "            if N_samples >= 4:\n",
    "                break\n",
    "    if N_samples >=4:\n",
    "        break\n",
    "        \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
